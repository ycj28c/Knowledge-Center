直播技术的细节

## 直播服务架构图： 
略，看[关于直播，所有的技术细节都在这里了（四）](https://zhuanlan.zhihu.com/p/23717383)

物理硬件上关键的性能提升就是靠CDN，起到一个cache的作用，所有主播的上传和用户的下载播放都需要CDN进行加速分发。还提到了一个技术点叫做BGP中转，相当于把联通和移动进行短途连接，减少hop。
软件层面就是通过视频解码优化，以及保证关键帧来确保流畅度。

## 直播的协议
有一些比较流行的协议：
RTMP：是Adobe的FlashPlayer使用的专利协议，大概延迟2-5秒  
HLS： 就是Http Live Streaming，是苹果提出的流媒体传输协议。最大的优点是HTML5打开即播，适用性广。延迟相对较高，可能5-7秒  
HDL（HTTP-FLV)：和RTMP类似，但是不会Adobe专利影响，延迟甚至好于RTMP  
RTP：Real-time Transport Protocol，就是典型的UDP协议，延迟最优，但是丢包  

如果能解决HDL的延迟，肯定是最优先选择，这需要主流浏览器更好的支持新协议WebRTC和WebSocket

## 视频基础知识
视频的几个步骤：录制->编码->网络传输->解码->播放，每个环节都产生延迟。怎么实现秒播？

视频由很多帧组成：
1.I帧表示关键帧。你可以理解为这一帧画面的完整保留；解码时只需要本帧数据就可以完成。（因为包含完整画面）  
2.P帧表示这一帧跟之前的一个关键帧（或P帧）的差别。解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧的画面差别的数据）  
3。B帧是双向差别帧。B帧记录的是本帧与前后帧的差别（具体比较复杂，有4种情况）。换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面。  

一个典型的视频帧序列为IBBPBBPBBP……对于直播而言，为了减少直播的延时，通常在编码时不使用B帧。P帧B帧对于I帧都有直接或者间接的依赖关系，所以播放器要解码一个视频帧序列，并进行播放，必须首先解码出I帧，其后续的B帧和P帧才能进行解码。比较好的策略是服务端自动判断关键帧的间隔，按业务需求缓存帧序列，保证在缓存中存储至少两个或者以上的关键帧，以应对低延时、防卡顿、智能丢包等需求。

网络好无所谓，网络差就要考虑主动丢包。后台采用逐步丢帧的策略，每个视频帧序列，丢最后的一到两帧，使得用户的感知最小，平滑的逐步缩小延时的效果。

## 播放器架构
目前有很多端设备，PC，mobile等等，播放器也是五花八门。  

底层核心：大多是C，C++写的视频编码，数据接收模块/解复用模块/音视频解码、滤镜、渲染模块/字幕/应用层结合功能：DRM、多码率/统计等。这一块拿现成的来用就行。

应用层：就是各种语言了，objectC，Java，C#等等，I/统计/DRM（数字版权保护）/多码率/弹幕/广告等。  

主要说一下应用层各平台：
HTML5：大多平台适用，使用Javascript，HTML/CSS技术，使用HTML5 Video默认播放器，MSE（Media Source Extensions）是主流浏览器的Web API，可以方便的使用Javascript进行封装，转码等。

Flash:随着HTML5的流行，淡出，使用Web-Flash技术比如ActionScript，需要浏览器安装FlashPlayer

IOS：Iphone，ipad等，原生APP通常用原生的ObjectC，Swift语言开发端。自带了AVplayer播放器，MPMovePlaerController封装API，有VideoToolbox硬编码，配合FFmpeg软编码

Android：包括Andoird phone，pad，TV，Box等，原生APP用原生Java，JNI开发。平台提供了MediaPlayer默认播放器和MediaCodec硬编码类，配合FFmpe引擎。

## 多媒体引擎
包括很多细节：
1、数据接收（Source）  
作为数据入口，这里的文件可以通过本地文件（File://协议开头）输入，也可以通过网络协议（如HTTP、RTMP、RTSP等）输入。在找到对应的传输协议之后，FFmpeg会根据协议特性与本机或服务器建立连接并获取到流数据。

2、解复用（Demux）  
通过对文件的特征码分析，可以找到文件的封装格式，如常见的MP4、FLV、TS、AVI等。根据封装格式的标准对其拆封，可以得到编码的音视频数据，一般称之为“packet”。

3、解码（Decode）  
解码器初始化时，利用之前源数据分析获得的音视频信息，分别设置对应的音频解码器和视频解码器。目前互联网中比较常见的音频编码方式有AAC（Advanced Audio Coding）、MP3，视频编码方式有H.264、H.265。对packet分别进行解码后，音频解码获得的数据是PCM（Pulse Code Modulation，脉冲编码调制）采样数据，一般称为“sample”。视频解码获得的数据是一幅YUV或RGB图像数据，一般称为“picture”。

4、音视频同步（Synchronizing）  
音视频解码时是两个独立线程，因此获得的音视频数据是分开的，并无任何关联。理想状态下，音视频按照自己固有频率渲染输出就能达到音视频同步的效果。但是在现实中，断网、弱网、丢帧、缓冲、音视频不同的解码耗时等情况都会妨碍实现同步，很难达到预期效果，所以要保证视频内容和音频内容对得上，必须做音视频同步。

5、音视频渲染（Render）  
经过音视频同步调整之后，需要把sample和picture分别输送给声卡和显卡，这部分工作建议由成熟的库来完成。常见的音频库有SDL、OpenAL/OpenAL ES、DirectSound、ALSA（Advanced Linux Sound Architecture）、OSS（Open Source System）等；视频库有SDL、OpenGL/OpenGL ES、DirectDraw、FrameBuffer等。

## Reference
[关于直播，所有的技术细节都在这里了（一）](https://zhuanlan.zhihu.com/p/23090320)
[关于直播，所有的技术细节都在这里了（二）](https://zhuanlan.zhihu.com/p/23377305)
[关于直播，所有的技术细节都在这里了（三）](https://zhuanlan.zhihu.com/p/23531863)
[关于直播，所有的技术细节都在这里了（四）](https://zhuanlan.zhihu.com/p/23717383)